{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":113155,"databundleVersionId":13473948,"sourceType":"competition"},{"sourceId":13507351,"sourceType":"datasetVersion","datasetId":8576037},{"sourceId":442871,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":359690,"modelId":380893}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/andyc97/chemberta-finetuning-with-downstream-xgboost?scriptVersionId=272629735\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# RDKit: Open-Source Cheminformatics Software\nhttps://www.rdkit.org/\n\n%%capture\n!pip install rdkit","metadata":{},"attachments":{}},{"cell_type":"markdown","source":"# Libraries","metadata":{}},{"cell_type":"code","source":"import os\nfrom itertools import product\n\n# Basic data manipulation\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# RDKit for cheminformatics\nfrom rdkit import Chem\nfrom rdkit.Chem import Descriptors\nfrom rdkit.Chem.rdMolDescriptors import GetMorganFingerprintAsBitVect, GetMACCSKeysFingerprint\n\n# XG boost\nimport xgboost as xgb\n\n# Sklearn for downstream prediction\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\n\n# Pytorch for finetuning BERT model\nimport torch\nimport torch.nn as nn\nfrom torch.optim import AdamW\nfrom torch.utils.data import Dataset, DataLoader\n\n# Load transformer model\nfrom transformers import AutoModelForMaskedLM, AutoTokenizer, AutoModel, AutoModelForSequenceClassification","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T12:51:09.950235Z","iopub.execute_input":"2025-11-01T12:51:09.950767Z","iopub.status.idle":"2025-11-01T12:51:20.599166Z","shell.execute_reply.started":"2025-11-01T12:51:09.950735Z","shell.execute_reply":"2025-11-01T12:51:20.598366Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"markdown","source":"## Load Data\n* The training dataset `train.csv` is loaded to `df_train`.\n* The test dataset `test.csv` is loaded to `df_test`.\n* The augmented dataset `smiles_melting_point.csv` is loaded to `df_aug` (https://www.kaggle.com/datasets/seddiktrk/melting-point-smiles).","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/melting-point/train.csv')\ndf_test = pd.read_csv('/kaggle/input/melting-point/test.csv')\ndf_aug = pd.read_csv('/kaggle/input/melting-point-smiles/smiles_melting_point.csv') # external dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T12:52:06.598469Z","iopub.execute_input":"2025-11-01T12:52:06.599172Z","iopub.status.idle":"2025-11-01T12:52:07.832513Z","shell.execute_reply.started":"2025-11-01T12:52:06.599137Z","shell.execute_reply":"2025-11-01T12:52:07.831716Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Handle Data Leakage","metadata":{}},{"cell_type":"markdown","source":"+ Convert smiles strings in the test set to its caninical form.","metadata":{}},{"cell_type":"code","source":"# Helper function\ndef make_canonical(smile):\n    mol = Chem.MolFromSmiles(smile)\n    if mol is not None:\n        canonical_smile = Chem.MolToSmiles(mol)\n        return canonical_smile\n    else:\n        return np.nan\n\n# The list of smile string to be exluded\nblack_list = df_test['SMILES'].apply(make_canonical).to_list()\n\n# Canonicalize smile string in the augmented dataset \ndf_aug['SMILES_C'] = df_aug['SMILES'].apply(make_canonical)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T13:09:41.039234Z","iopub.execute_input":"2025-11-01T13:09:41.039845Z","iopub.status.idle":"2025-11-01T13:10:44.674059Z","shell.execute_reply.started":"2025-11-01T13:09:41.039822Z","shell.execute_reply":"2025-11-01T13:10:44.67325Z"}},"outputs":[{"name":"stderr","text":"[13:10:44] Explicit valence for atom # 13 Cl, 7, is greater than permitted\n[13:10:44] Explicit valence for atom # 32 Cl, 5, is greater than permitted\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"df_train_aug = df_aug[(~df_aug['SMILES_C'].isin(black_list)) & (df_aug['UNIT {Melting Point}.1'] == 'K')]\ndf_train_aug = df_train_aug[['SMILES', 'Melting Point {measured, converted}']]\n\n# Rename column to align with competition dataset\ndf_train_aug.columns = ['SMILES', 'Tm']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T13:21:26.257766Z","iopub.execute_input":"2025-11-01T13:21:26.258498Z","iopub.status.idle":"2025-11-01T13:21:26.329367Z","shell.execute_reply.started":"2025-11-01T13:21:26.258471Z","shell.execute_reply":"2025-11-01T13:21:26.328827Z"}},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":"## Data Augmentation\n+ Combining both datasets we have 276,510 training points.","metadata":{}},{"cell_type":"code","source":"df_train = pd.concat([df_train[['SMILES', 'Tm']], df_train_aug], ignore_index=True)\ndf_train.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T13:26:45.504807Z","iopub.execute_input":"2025-11-01T13:26:45.50533Z","iopub.status.idle":"2025-11-01T13:26:45.514567Z","shell.execute_reply.started":"2025-11-01T13:26:45.505308Z","shell.execute_reply":"2025-11-01T13:26:45.513895Z"}},"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"(276510, 2)"},"metadata":{}}],"execution_count":40},{"cell_type":"markdown","source":"## Train-Test Split\n+ The dataset is split into 80/20 for training and validation.","metadata":{}},{"cell_type":"code","source":"seed = 20251017\ntorch.manual_seed(seed)\ndf_train, df_val = train_test_split(df_train, test_size=0.2, random_state=seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T12:52:30.551762Z","iopub.execute_input":"2025-11-01T12:52:30.552503Z","iopub.status.idle":"2025-11-01T12:52:30.574041Z","shell.execute_reply.started":"2025-11-01T12:52:30.552471Z","shell.execute_reply":"2025-11-01T12:52:30.573197Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"+ Standardization $(y_i-\\bar{y})/s$ is applied to the target, melting points `Tm`.\n+ The standardized target values are denoted by `TmS`.","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit(df_train[['Tm']]) # pass in 2D array\nstd = scaler.var_[0]**0.5 # estimate of s\n\ndf_train['TmS'] = scaler.transform(df_train[['Tm']]).flatten() # convert back to 1D array \ndf_val['TmS'] = scaler.transform(df_val[['Tm']]).flatten()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T12:19:35.145056Z","iopub.execute_input":"2025-10-26T12:19:35.1456Z","iopub.status.idle":"2025-10-26T12:19:35.160378Z","shell.execute_reply.started":"2025-10-26T12:19:35.145582Z","shell.execute_reply":"2025-10-26T12:19:35.159729Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"+ Define the data handler for fine-tuning ChemBerta with pytorch.","metadata":{}},{"cell_type":"code","source":"class ChemDataset(Dataset):  \n    def __init__(self, df, tokenizer, max_length=128):  \n        self.smiles = df['SMILES'].tolist()  \n        self.labels = df['TmS'].tolist()  \n        self.tokenizer = tokenizer  \n        self.max_length = max_length  \n  \n    def __len__(self):  \n        return len(self.labels)  \n  \n    def __getitem__(self, idx):  \n        encoding = self.tokenizer(  \n            self.smiles[idx],  \n            truncation=True,  \n            padding='max_length',  \n            max_length=self.max_length,  \n            return_tensors='pt'  \n        )  \n        item = {key: val.squeeze(0) for key, val in encoding.items()}  \n        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)  \n        return item  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T12:19:35.161097Z","iopub.execute_input":"2025-10-26T12:19:35.161318Z","iopub.status.idle":"2025-10-26T12:19:35.16867Z","shell.execute_reply.started":"2025-10-26T12:19:35.161295Z","shell.execute_reply":"2025-10-26T12:19:35.167871Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"+ Define the transformer model ChemBerta.\n+ A regression head is added on top of the transformer model.","metadata":{}},{"cell_type":"code","source":"# Model retrieved from https://www.kaggle.com/code/michaelrowen/opp2025-chemberta-pre-trained-base\nclass BERTEmbedder:\n    def __init__(self, model_name):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForSequenceClassification.from_pretrained(  \n            model_name,\n            num_labels=1,  \n            problem_type='regression' # Regression task  \n        )  \n        self.model.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T12:19:35.169708Z","iopub.execute_input":"2025-10-26T12:19:35.170037Z","iopub.status.idle":"2025-10-26T12:19:35.182535Z","shell.execute_reply.started":"2025-10-26T12:19:35.170014Z","shell.execute_reply":"2025-10-26T12:19:35.18184Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Finetuning ChemBERTa","metadata":{}},{"cell_type":"markdown","source":"+ The transformer model `ChemBerta` is available in Kaggle (https://www.kaggle.com/code/michaelrowen/opp2025-chemberta-pre-trained-base).\n+ Load the transfomer model and define data handlers for training and validation sets, respectively.","metadata":{}},{"cell_type":"code","source":"# Training configuration\nchemberta_model = '/kaggle/input/c/transformers/default/1/ChemBERTa-77M-MLM'\nchemberta = BERTEmbedder(model_name=chemberta_model)\noptimizer = AdamW(chemberta.model.parameters(), lr=1e-4) # lr set by experiment\nloss_fn = nn.L1Loss() # Since reduce = mean, L1Loss measures MAE\nn_epochs = 30\n\n# Data handler\ndataset_train = ChemDataset(df_train, chemberta.tokenizer)\ndataset_val = ChemDataset(df_val, chemberta.tokenizer)\ndataloader_train = DataLoader(dataset_train, batch_size=16, shuffle=True) # batch_size set by experiment\ndataloader_val = DataLoader(dataset_val, batch_size=16, shuffle=True) # batch_size set by experiment","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T12:19:35.183322Z","iopub.execute_input":"2025-10-26T12:19:35.183516Z","iopub.status.idle":"2025-10-26T12:19:50.736548Z","shell.execute_reply.started":"2025-10-26T12:19:35.183502Z","shell.execute_reply":"2025-10-26T12:19:50.736011Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"+ Move the model to GPU on Kaggle platform and start training.","metadata":{}},{"cell_type":"code","source":"if not os.path.exists('/kaggle/working/weights.pth'):\n    # Enable GPU if available\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'  \n    chemberta.model.to(device)\n    \n    train_loss_list = []\n    val_loss_list = []\n    # Training cycle\n    for epoch in range(n_epochs):\n        chemberta.model.train()\n        epoch_train_size = 0\n        epoch_val_size = 0\n        epoch_train_loss = 0\n        epoch_val_loss = 0\n    \n        # evaluate training set\n        for batch in dataloader_train:\n            # forward pass\n            inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}  \n            labels = batch['labels'].to(device).unsqueeze(1)  # shape [B,1]  \n            outputs = chemberta.model(**inputs).logits  # shape [B,1] \n            loss = loss_fn(outputs, labels)  \n    \n            # backward pass\n            optimizer.zero_grad()  \n            loss.backward()  \n            optimizer.step()  \n    \n            # update epoch loss\n            epoch_train_loss += loss.item() * dataloader_train.batch_size * std\n            epoch_train_size += dataloader_train.batch_size\n        \n        # evaluate validation set\n        with torch.no_grad():\n            for batch in dataloader_val:\n                # forward pass\n                inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}  \n                labels = batch['labels'].to(device).unsqueeze(1)  # shape [B,1]  \n                outputs = chemberta.model(**inputs).logits  # shape [B,1] \n                loss = loss_fn(outputs, labels)  \n    \n                # update epoch loss\n                epoch_val_loss += loss.item() * dataloader_val.batch_size * std\n                epoch_val_size += dataloader_val.batch_size\n    \n        # compute loss per epoch\n        avg_train_loss = epoch_train_loss/epoch_train_size\n        avg_val_loss = epoch_val_loss/epoch_val_size\n    \n        # save model with the lowest validation loss\n        if len(val_loss_list) > 0 and avg_val_loss < np.min(val_loss_list):\n            torch.save(chemberta.model.state_dict(), '/kaggle/working/weights.pth')\n            \n        train_loss_list.append(avg_train_loss)\n        val_loss_list.append(avg_val_loss)\n        print(f\"Epoch {epoch + 1} done, training loss: {avg_train_loss:.4f}, validation loss: {avg_val_loss:.4f}\") \n        \n    # Evaluate in CPU\n    chemberta.model.eval()\n    chemberta.model.to('cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T12:19:50.73723Z","iopub.execute_input":"2025-10-26T12:19:50.737771Z","iopub.status.idle":"2025-10-26T12:21:18.734205Z","shell.execute_reply.started":"2025-10-26T12:19:50.737741Z","shell.execute_reply":"2025-10-26T12:21:18.733516Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the best model\nchemberta.model.load_state_dict(torch.load('/kaggle/working/weights.pth', map_location=torch.device('cpu')))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T12:21:18.734929Z","iopub.execute_input":"2025-10-26T12:21:18.735134Z","iopub.status.idle":"2025-10-26T12:21:18.763565Z","shell.execute_reply.started":"2025-10-26T12:21:18.735119Z","shell.execute_reply":"2025-10-26T12:21:18.762974Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"+ The validation loss seems to be converged after 15 epochs. ","metadata":{}},{"cell_type":"code","source":"plt.plot([i + 1 for i in range(n_epochs)], train_loss_list, label ='training loss')\nplt.plot([i + 1 for i in range(n_epochs)], val_loss_list, '-.', label ='validation loss')\n\nplt.xlabel(\"X-axis data\")\nplt.ylabel(\"Y-axis data\")\nplt.legend()\nplt.title('Finetuning ChemBerta with Regression layer')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T12:21:18.764337Z","iopub.execute_input":"2025-10-26T12:21:18.76454Z","iopub.status.idle":"2025-10-26T12:21:19.048347Z","shell.execute_reply.started":"2025-10-26T12:21:18.764525Z","shell.execute_reply":"2025-10-26T12:21:19.047536Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Generate Features","metadata":{}},{"cell_type":"code","source":"def extract_chembert_embeddings(smiles_list, embedder, n_data):\n    n_latent = 384\n    embeddings = np.zeros((n_data, n_latent))\n    \n    for i, smiles in enumerate(smiles_list):\n        with torch.no_grad():\n            # Getting the model output\n            encoded_input = embedder.tokenizer(smiles, return_tensors='pt', padding=True, truncation=True)\n            model_output = embedder.model(**encoded_input, output_hidden_states=True)\n            # embeddings[i, :] = model_output.logits.numpy()\n            \n            # Getting the CLS token from model output\n            embedding = model_output.hidden_states[3][:, 0, :]\n            embeddings[i, :] = embedding.numpy()\n    \n    return pd.DataFrame(embeddings, columns=[f'embedding_{i+1}' for i in range(n_latent)])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T12:21:19.048987Z","iopub.execute_input":"2025-10-26T12:21:19.049168Z","iopub.status.idle":"2025-10-26T12:21:19.054305Z","shell.execute_reply.started":"2025-10-26T12:21:19.049155Z","shell.execute_reply":"2025-10-26T12:21:19.053522Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Code retrieved from https://www.kaggle.com/code/aliffaagnur/single-lightgbm-optuna/notebook\ndef extract_all_descriptors(smiles_list):\n\n    # GET ALL DESCRIPTORS\n    descriptor_list = Descriptors._descList    # --> THESE WILL RETURN LIST OF TUPLE\n    descriptors = [desc[0] for desc in descriptor_list]\n\n    print(f'There Are {len(descriptor_list)} Descriptor Features')\n\n    # EXTRACT ALL DESCRIPTORS FROM SMILES FEATURES\n    result = []\n    for smiles in smiles_list:\n\n        mol = Chem.MolFromSmiles(smiles)\n\n        # IF MOLECOLE IS INVALID\n        if mol is None:\n            row = {name : None for name, func in descriptor_list}\n        else:\n            # CREATE DESCRIPTORS FEATURES\n            row = {name: func(mol) for name, func in descriptor_list}\n\n        result.append(row)\n\n    # MERGE DATA WITH EXTRACTED FEATURES\n    return pd.DataFrame(result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T12:21:19.055434Z","iopub.execute_input":"2025-10-26T12:21:19.055673Z","iopub.status.idle":"2025-10-26T12:21:19.066593Z","shell.execute_reply.started":"2025-10-26T12:21:19.05565Z","shell.execute_reply":"2025-10-26T12:21:19.065862Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_fingerprints(smiles_list):\n    \"\"\"\n    Generates all features for a list of SMILES strings using RDKit.\n    Returns a list of feature dictionaries.\n    \"\"\"\n    features = []\n    for smiles in smiles_list:\n        mol = Chem.MolFromSmiles(smiles)\n        \n        feature_dict = {}\n        if mol is not None:\n            # Descriptors\n            for name, func in Descriptors.descList:\n                try:\n                    feature_dict[name] = func(mol)\n                except Exception:\n                    feature_dict[name] = np.nan\n            \n            # Morgan Fingerprint \n            fp_morgan = GetMorganFingerprintAsBitVect(mol, radius=2, nBits=1024)\n            for i in range(fp_morgan.GetNumBits()):\n                feature_dict[f\"Morgan_{i}\"] = int(fp_morgan.GetBit(i))\n            \n            # MACCS Keys\n            fp_maccs = GetMACCSKeysFingerprint(mol)\n            for i in range(fp_maccs.GetNumBits()):\n                feature_dict[f\"MACCS_{i}\"] = int(fp_maccs.GetBit(i))\n\n        features.append(feature_dict)\n    return pd.DataFrame(features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T13:07:11.940198Z","iopub.execute_input":"2025-10-26T13:07:11.9405Z","iopub.status.idle":"2025-10-26T13:07:11.946178Z","shell.execute_reply.started":"2025-10-26T13:07:11.94048Z","shell.execute_reply":"2025-10-26T13:07:11.94543Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%capture\n# Generate features for training set\nembeddings_train = extract_chembert_embeddings(df_train['SMILES'], chemberta, df_train.shape[0])\nmolecular_features_train = extract_all_descriptors(df_train['SMILES'])\nmolecular_fingerprint_train = extract_fingerprints(df_train['SMILES'])\n\n# Generate features for validation set\nembeddings_val = extract_chembert_embeddings(df_val['SMILES'], chemberta, df_val.shape[0])\nmolecular_features_val = extract_all_descriptors(df_val['SMILES'])\nmolecular_fingerprint_val = extract_fingerprints(df_val['SMILES'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T13:27:43.777239Z","iopub.execute_input":"2025-10-26T13:27:43.777841Z","iopub.status.idle":"2025-10-26T13:28:26.324177Z","shell.execute_reply.started":"2025-10-26T13:27:43.777822Z","shell.execute_reply":"2025-10-26T13:28:26.323374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dataset size\nprint('Training set')\nprint(embeddings_train.shape, type(embeddings_train))\nprint(molecular_features_train.shape, type(molecular_features_train))\nprint(molecular_fingerprint_train.shape, type(molecular_fingerprint_train))\n\nprint('Valiodation set')\nprint(embeddings_val.shape, type(embeddings_val))\nprint(molecular_features_val.shape, type(molecular_features_val))\nprint(molecular_fingerprint_val.shape, type(molecular_fingerprint_val))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T13:28:59.740275Z","iopub.execute_input":"2025-10-26T13:28:59.741076Z","iopub.status.idle":"2025-10-26T13:28:59.747075Z","shell.execute_reply.started":"2025-10-26T13:28:59.741044Z","shell.execute_reply":"2025-10-26T13:28:59.746247Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Downstream Prediction Task","metadata":{}},{"cell_type":"code","source":"# Dataset for sklearn - Training set\ndf_ttl_train = pd.concat([\n    df_train.reset_index(drop=True), \n    embeddings_train, \n    # molecular_features_train,\n    molecular_fingerprint_train\n], axis=1)\ny_train = df_ttl_train['TmS']\nX_train = df_ttl_train.drop(df_train.columns, axis=1)\n#X_train = df_ttl_train.drop(['id', 'Tm', 'TmS', 'SMILES'], axis=1)\nX_train.columns = [str(colname) for colname in X_train.columns]\n\n# Dataset for sklearn - Validation set\ndf_ttl_val = pd.concat([\n    df_val.reset_index(drop=True), \n    embeddings_val, \n    # molecular_features_val,\n    molecular_fingerprint_val\n], axis=1)\ny_val = df_ttl_val['TmS']\nX_val = df_ttl_val.drop(df_val.columns, axis=1)\n#X_val = df_ttl_val.drop(['id', 'Tm', 'TmS', 'SMILES'], axis=1)\nX_val.columns = [str(colname) for colname in X_val.columns]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T13:30:24.562803Z","iopub.execute_input":"2025-10-26T13:30:24.56362Z","iopub.status.idle":"2025-10-26T13:30:24.603334Z","shell.execute_reply.started":"2025-10-26T13:30:24.563586Z","shell.execute_reply":"2025-10-26T13:30:24.602368Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Extreme Gradient Boosting","metadata":{}},{"cell_type":"code","source":"# Define hyperparameter grid for optimization  \nparam_grid = {\n    'n_estimators' : [10000],\n    'max_depth': [7, 9, 12],\n    'eta': [0.02, 0.05, 0.1],\n    'subsample': [0.5, 0.7],\n    'colsample_bytree': [0.5, 0.7],\n    'reg_alpha': [0, 0.5, 0.7],\n    'reg_lambda': [0, 0.5, 0.7]\n}\nparam_list = product(*[param_grid[key] for key in param_grid.keys()])\nparam_names = [*param_grid.keys()]\n\n# Define Dmatrix for GPU training\n# dtrain = xgb.DMatrix(X_train, label=y_train)\n# dval = xgb.DMatrix(X_val, label=y_val)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T14:19:43.925843Z","iopub.execute_input":"2025-10-26T14:19:43.926433Z","iopub.status.idle":"2025-10-26T14:19:43.93101Z","shell.execute_reply.started":"2025-10-26T14:19:43.926412Z","shell.execute_reply":"2025-10-26T14:19:43.930265Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mae_scores = []\nfor i, param in enumerate(param_list):\n    param_dict = dict(zip(param_names, param))\n    model_xgb = xgb.XGBRegressor(random_state=seed, objective='reg:squarederror', eval_metric='mae', early_stopping_rounds=100, device='cuda', **param_dict)\n    model_xgb.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], verbose=False)\n    result = model_xgb.evals_result()\n    train_loss = result['validation_0']['mae'][-1] * std\n    val_loss = result['validation_1']['mae'][-1] * std\n    print(f'Model {i}', '\\t', train_loss, '\\t', val_loss)\n    mae_scores.append(list(param) + [val_loss])\n    \ndf_downstream = pd.DataFrame(mae_scores, columns= param_names + ['mae_score']) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T14:19:45.039742Z","iopub.execute_input":"2025-10-26T14:19:45.040197Z","iopub.status.idle":"2025-10-26T14:23:12.918185Z","shell.execute_reply.started":"2025-10-26T14:19:45.040176Z","shell.execute_reply":"2025-10-26T14:23:12.91727Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_downstream = pd.DataFrame(mae_scores, columns= param_names + ['mae_score']) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T12:43:34.937733Z","iopub.execute_input":"2025-10-26T12:43:34.938001Z","iopub.status.idle":"2025-10-26T12:43:34.942886Z","shell.execute_reply.started":"2025-10-26T12:43:34.937984Z","shell.execute_reply":"2025-10-26T12:43:34.942076Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_downstream.sort_values(by='mae_score', inplace=True)\ndf_downstream.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T12:43:36.158607Z","iopub.execute_input":"2025-10-26T12:43:36.158877Z","iopub.status.idle":"2025-10-26T12:43:36.18442Z","shell.execute_reply.started":"2025-10-26T12:43:36.158858Z","shell.execute_reply":"2025-10-26T12:43:36.183816Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Retrieve the best model parameters\nbest_model_config = df_downstream.nsmallest(1, 'mae_score').\\\n    drop('mae_score', axis=1).\\\n    iloc[0, :].to_dict()\n\n# Type correction\nbest_model_config['max_depth'] = int(best_model_config['max_depth'])\nbest_model_config['n_estimators'] = int(best_model_config['n_estimators'])\n\n# Train the downstream model with the best parameters\nbest_model = xgb.XGBRegressor(random_state=seed, **best_model_config)\nbest_model.fit(X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T12:43:51.197317Z","iopub.execute_input":"2025-10-26T12:43:51.197588Z","iopub.status.idle":"2025-10-26T12:44:46.286381Z","shell.execute_reply.started":"2025-10-26T12:43:51.197568Z","shell.execute_reply":"2025-10-26T12:44:46.285618Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"# Generate features for the test set\nembeddings_test = extract_chembert_embeddings(df_test['SMILES'], chemberta, df_test.shape[0])\nmolecular_features_test = extract_all_descriptors(df_test['SMILES'])\n\n# Prepare dataset for downstream regression tasks\ndf_ttl_test = pd.concat([\n    df_test.reset_index(drop=True), \n    embeddings_test, \n    molecular_features_test\n], axis=1)\n\n\nX_test = df_ttl_test.drop(df_test.columns, axis=1)\nX_test.columns = [str(colname) for colname in X_test.columns]\n\ny_pred = scaler.inverse_transform( # back transform to raw target\n    np.expand_dims(best_model.predict(X_test), axis=1) # convert to 2D array by adding an extra dimension\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T12:44:56.126433Z","iopub.execute_input":"2025-10-26T12:44:56.127025Z","iopub.status.idle":"2025-10-26T12:45:02.978612Z","shell.execute_reply.started":"2025-10-26T12:44:56.127002Z","shell.execute_reply":"2025-10-26T12:45:02.977795Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_out = pd.DataFrame({'id': df_ttl_test['id'],'Tm': y_pred.flatten()})\ndf_out.to_csv('./submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T12:45:06.279616Z","iopub.execute_input":"2025-10-26T12:45:06.280341Z","iopub.status.idle":"2025-10-26T12:45:06.290459Z","shell.execute_reply.started":"2025-10-26T12:45:06.280317Z","shell.execute_reply":"2025-10-26T12:45:06.289679Z"}},"outputs":[],"execution_count":null}]}