{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":113155,"databundleVersionId":13473948,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":13523236,"sourceType":"datasetVersion","datasetId":8586744},{"sourceId":442871,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":359690,"modelId":380893}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/andyc97/chemberta-finetuning-with-downstream-xgboost?scriptVersionId=272623148\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Packages","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install rdkit","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T14:31:21.785331Z","iopub.execute_input":"2025-10-23T14:31:21.785754Z","iopub.status.idle":"2025-10-23T14:31:25.044983Z","shell.execute_reply.started":"2025-10-23T14:31:21.78573Z","shell.execute_reply":"2025-10-23T14:31:25.043882Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Basic data manipulation\nimport numpy as np\nimport pandas as pd\n\n# RDKit for cheminformatics\nfrom rdkit import Chem\nfrom rdkit.Chem import Descriptors\n\n# XG boost\nimport xgboost as xgb\n\n# Sklearn for downstream prediction\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\n\n# Pytorch for finetuning BERT model\nimport torch\nimport torch.nn as nn\nfrom torch.optim import AdamW\nfrom torch.utils.data import Dataset, DataLoader\n\n# Load transformer model\nfrom transformers import AutoModelForMaskedLM, AutoTokenizer, AutoModel, AutoModelForSequenceClassification","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T14:31:25.046838Z","iopub.execute_input":"2025-10-23T14:31:25.04722Z","iopub.status.idle":"2025-10-23T14:31:25.052204Z","shell.execute_reply.started":"2025-10-23T14:31:25.047198Z","shell.execute_reply":"2025-10-23T14:31:25.051504Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset and Pretrained Model","metadata":{}},{"cell_type":"markdown","source":"* The training dataset `train.csv` is loaded to `df_train`.\n* The transformer model `ChemBerta` is available in Kaggle (https://www.kaggle.com/code/michaelrowen/opp2025-chemberta-pre-trained-base).","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/melting-point/train.csv')\ndf_test = pd.read_csv('/kaggle/input/melting-point/test.csv')\nchemberta_model = '/kaggle/input/c/transformers/default/1/ChemBERTa-77M-MLM'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T14:31:25.053178Z","iopub.execute_input":"2025-10-23T14:31:25.053362Z","iopub.status.idle":"2025-10-23T14:31:25.184265Z","shell.execute_reply.started":"2025-10-23T14:31:25.05335Z","shell.execute_reply":"2025-10-23T14:31:25.183382Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"+ The dataset is split into 80/20 for training and validation.","metadata":{}},{"cell_type":"code","source":"seed = 20251017\ntorch.manual_seed(seed)\ndf_train, df_val = train_test_split(df_train, test_size=0.2, random_state=seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T14:31:25.186312Z","iopub.execute_input":"2025-10-23T14:31:25.186815Z","iopub.status.idle":"2025-10-23T14:31:25.199478Z","shell.execute_reply.started":"2025-10-23T14:31:25.186793Z","shell.execute_reply":"2025-10-23T14:31:25.198822Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"+ Standardization $(y_i-\\bar{y})/s$ is applied to the target, melting points `Tm`.\n+ The standardized target values are denoted by `TmS`.","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit(df_train[['Tm']]) # pass in 2D array\nstd = scaler.var_[0]**0.5 # estimate of s\n\ndf_train['TmS'] = scaler.transform(df_train[['Tm']]).flatten() # convert back to 1D array \ndf_val['TmS'] = scaler.transform(df_val[['Tm']]).flatten()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T14:31:25.200373Z","iopub.execute_input":"2025-10-23T14:31:25.200668Z","iopub.status.idle":"2025-10-23T14:31:25.212657Z","shell.execute_reply.started":"2025-10-23T14:31:25.200644Z","shell.execute_reply":"2025-10-23T14:31:25.211871Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"+ Define the data handler for fine-tuning ChemBerta with pytorch.","metadata":{}},{"cell_type":"code","source":"class ChemDataset(Dataset):  \n    def __init__(self, df, tokenizer, max_length=128):  \n        self.smiles = df['SMILES'].tolist()  \n        self.labels = df['TmS'].tolist()  \n        self.tokenizer = tokenizer  \n        self.max_length = max_length  \n  \n    def __len__(self):  \n        return len(self.labels)  \n  \n    def __getitem__(self, idx):  \n        encoding = self.tokenizer(  \n            self.smiles[idx],  \n            truncation=True,  \n            padding='max_length',  \n            max_length=self.max_length,  \n            return_tensors='pt'  \n        )  \n        item = {key: val.squeeze(0) for key, val in encoding.items()}  \n        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)  \n        return item  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T14:31:25.213679Z","iopub.execute_input":"2025-10-23T14:31:25.214393Z","iopub.status.idle":"2025-10-23T14:31:25.229193Z","shell.execute_reply.started":"2025-10-23T14:31:25.214373Z","shell.execute_reply":"2025-10-23T14:31:25.228497Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"+ Define the transformer model ChemBerta.\n+ A regression head is added on top of the transformer model.","metadata":{}},{"cell_type":"code","source":"# Model retrieved from https://www.kaggle.com/code/michaelrowen/opp2025-chemberta-pre-trained-base\nclass BERTEmbedder:\n    def __init__(self, model_name):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForSequenceClassification.from_pretrained(  \n            model_name,\n            num_labels=1,  \n            problem_type='regression' # Regression task  \n        )  \n        self.model.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T14:31:25.229917Z","iopub.execute_input":"2025-10-23T14:31:25.230133Z","iopub.status.idle":"2025-10-23T14:31:25.241362Z","shell.execute_reply.started":"2025-10-23T14:31:25.230117Z","shell.execute_reply":"2025-10-23T14:31:25.240703Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Finetuning ChemBERTA","metadata":{}},{"cell_type":"markdown","source":"+ Load the transfomer model and define data handlers for training and validation sets, respectively.","metadata":{}},{"cell_type":"code","source":"# Training configuration\nchemberta = BERTEmbedder(model_name=chemberta_model)\noptimizer = AdamW(chemberta.model.parameters(), lr=1e-4) # lr set by experiment\nloss_fn = nn.L1Loss() # Since reduce = mean, L1Loss measures MAE\nn_epochs = 30\n\n# Data handler\ndataset_train = ChemDataset(df_train, chemberta.tokenizer)\ndataset_val = ChemDataset(df_val, chemberta.tokenizer)\ndataloader_train = DataLoader(dataset_train, batch_size=16, shuffle=True) # batch_size set by experiment\ndataloader_val = DataLoader(dataset_val, batch_size=16, shuffle=True) # batch_size set by experiment","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T14:31:25.242288Z","iopub.execute_input":"2025-10-23T14:31:25.242547Z","iopub.status.idle":"2025-10-23T14:31:40.731475Z","shell.execute_reply.started":"2025-10-23T14:31:25.242525Z","shell.execute_reply":"2025-10-23T14:31:40.730821Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"+ Move the model to GPU on Kaggle platform and start training.","metadata":{}},{"cell_type":"code","source":"# Enable GPU if available\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'  \nchemberta.model.to(device)\n\ntrain_loss_list = []\nval_loss_list = []\n# Training cycle\nfor epoch in range(n_epochs):\n    chemberta.model.train()\n    epoch_train_size = 0\n    epoch_val_size = 0\n    epoch_train_loss = 0\n    epoch_val_loss = 0\n\n    # evaluate training set\n    for batch in dataloader_train:\n        # forward pass\n        inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}  \n        labels = batch['labels'].to(device).unsqueeze(1)  # shape [B,1]  \n        outputs = chemberta.model(**inputs).logits  # shape [B,1] \n        loss = loss_fn(outputs, labels)  \n\n        # backward pass\n        optimizer.zero_grad()  \n        loss.backward()  \n        optimizer.step()  \n\n        # update epoch loss\n        epoch_train_loss += loss.item() * dataloader_train.batch_size * std\n        epoch_train_size += dataloader_train.batch_size\n    \n    # evaluate validation set\n    with torch.no_grad():\n        for batch in dataloader_val:\n            # forward pass\n            inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}  \n            labels = batch['labels'].to(device).unsqueeze(1)  # shape [B,1]  \n            outputs = chemberta.model(**inputs).logits  # shape [B,1] \n            loss = loss_fn(outputs, labels)  \n\n            # update epoch loss\n            epoch_val_loss += loss.item() * dataloader_val.batch_size * std\n            epoch_val_size += dataloader_val.batch_size\n\n    # compute loss per epoch\n    avg_train_loss = epoch_train_loss/epoch_train_size\n    avg_val_loss = epoch_val_loss/epoch_val_size\n\n    # save model with the lowest validation loss\n    if len(val_loss_list) > 0 and avg_val_loss < np.min(val_loss_list):\n        torch.save(chemberta.model.state_dict(), '/kaggle/working/weights.pth')\n        \n    train_loss_list.append(avg_train_loss)\n    val_loss_list.append(avg_val_loss)\n    print(f\"Epoch {epoch + 1} done, training loss: {avg_train_loss:.4f}, validation loss: {avg_val_loss:.4f}\")  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T14:31:40.732592Z","iopub.execute_input":"2025-10-23T14:31:40.733175Z","iopub.status.idle":"2025-10-23T14:33:10.627886Z","shell.execute_reply.started":"2025-10-23T14:31:40.733155Z","shell.execute_reply":"2025-10-23T14:33:10.627255Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the best model\nchemberta.model.eval()\nchemberta.model.to('cpu')\nchemberta.model.load_state_dict(torch.load('/kaggle/working/weights.pth', map_location=torch.device('cpu')))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T14:33:10.62979Z","iopub.execute_input":"2025-10-23T14:33:10.62999Z","iopub.status.idle":"2025-10-23T14:33:10.688659Z","shell.execute_reply.started":"2025-10-23T14:33:10.629968Z","shell.execute_reply":"2025-10-23T14:33:10.688057Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"+ The validation loss seems to be converged after 15 epochs. ","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot([i + 1 for i in range(n_epochs)], train_loss_list, label ='training loss')\nplt.plot([i + 1 for i in range(n_epochs)], val_loss_list, '-.', label ='validation loss')\n\nplt.xlabel(\"X-axis data\")\nplt.ylabel(\"Y-axis data\")\nplt.legend()\nplt.title('Finetuning ChemBerta with Regression layer')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T14:33:10.689238Z","iopub.execute_input":"2025-10-23T14:33:10.689422Z","iopub.status.idle":"2025-10-23T14:33:10.945408Z","shell.execute_reply.started":"2025-10-23T14:33:10.689406Z","shell.execute_reply":"2025-10-23T14:33:10.944615Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Generate Features","metadata":{}},{"cell_type":"code","source":"def extract_chembert_embeddings(smiles_list, embedder, n_data):\n    n_latent = 384\n    embeddings = np.zeros((n_data, n_latent))\n    \n    for i, smiles in enumerate(smiles_list):\n        with torch.no_grad():\n            # Getting the model output\n            encoded_input = embedder.tokenizer(smiles, return_tensors='pt', padding=True, truncation=True)\n            model_output = embedder.model(**encoded_input, output_hidden_states=True)\n            # embeddings[i, :] = model_output.logits.numpy()\n            \n            # Getting the CLS token from model output\n            embedding = model_output.hidden_states[3][:, 0, :]\n            embeddings[i, :] = embedding.numpy()\n    \n    return pd.DataFrame(embeddings, columns=[f'embedding_{i+1}' for i in range(n_latent)])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T14:33:10.946325Z","iopub.execute_input":"2025-10-23T14:33:10.946686Z","iopub.status.idle":"2025-10-23T14:33:10.951782Z","shell.execute_reply.started":"2025-10-23T14:33:10.946662Z","shell.execute_reply":"2025-10-23T14:33:10.951021Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Code retrieved from https://www.kaggle.com/code/aliffaagnur/single-lightgbm-optuna/notebook\ndef extract_all_descriptors(smiles_list):\n\n    # GET ALL DESCRIPTORS\n    descriptor_list = Descriptors._descList    # --> THESE WILL RETURN LIST OF TUPLE\n    descriptors = [desc[0] for desc in descriptor_list]\n\n    print(f'There Are {len(descriptor_list)} Descriptor Features')\n\n    # EXTRACT ALL DESCRIPTORS FROM SMILES FEATURES\n    result = []\n    for smiles in smiles_list:\n\n        mol = Chem.MolFromSmiles(smiles)\n\n        # IF MOLECOLE IS INVALID\n        if mol is None:\n            row = {name : None for name, func in descriptor_list}\n        else:\n            # CREATE DESCRIPTORS FEATURES\n            row = {name: func(mol) for name, func in descriptor_list}\n\n        result.append(row)\n\n    # MERGE DATA WITH EXTRACTED FEATURES\n    return pd.DataFrame(result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T14:33:10.95239Z","iopub.execute_input":"2025-10-23T14:33:10.952648Z","iopub.status.idle":"2025-10-23T14:33:10.966597Z","shell.execute_reply.started":"2025-10-23T14:33:10.952631Z","shell.execute_reply":"2025-10-23T14:33:10.965787Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate features for training set\nembeddings_train = extract_chembert_embeddings(df_train['SMILES'], chemberta, df_train.shape[0])\nmolecular_features_train = extract_all_descriptors(df_train['SMILES'])\n\n# Generate features for validation set\nembeddings_val = extract_chembert_embeddings(df_val['SMILES'], chemberta, df_val.shape[0])\nmolecular_features_val = extract_all_descriptors(df_val['SMILES'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T14:33:10.967503Z","iopub.execute_input":"2025-10-23T14:33:10.967794Z","iopub.status.idle":"2025-10-23T14:33:39.707013Z","shell.execute_reply.started":"2025-10-23T14:33:10.967777Z","shell.execute_reply":"2025-10-23T14:33:39.705977Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dataset size\nprint('Training set')\nprint(embeddings_train.shape, type(embeddings_train))\nprint(molecular_features_train.shape, type(molecular_features_train))\n\nprint('Valiodation set')\nprint(embeddings_val.shape, type(embeddings_val))\nprint(molecular_features_val.shape, type(molecular_features_val))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T14:33:39.708159Z","iopub.execute_input":"2025-10-23T14:33:39.708682Z","iopub.status.idle":"2025-10-23T14:33:39.714432Z","shell.execute_reply.started":"2025-10-23T14:33:39.708649Z","shell.execute_reply":"2025-10-23T14:33:39.713613Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Downstream Prediction Task","metadata":{}},{"cell_type":"code","source":"# Dataset for sklearn - Training set\ndf_ttl_train = pd.concat([\n    df_train.reset_index(drop=True), \n    embeddings_train, \n    molecular_features_train\n], axis=1)\ny_train = df_ttl_train['TmS']\nX_train = df_ttl_train.drop(df_train.columns, axis=1)\n#X_train = df_ttl_train.drop(['id', 'Tm', 'TmS', 'SMILES'], axis=1)\nX_train.columns = [str(colname) for colname in X_train.columns]\n\n# Dataset for sklearn - Validation set\ndf_ttl_val = pd.concat([\n    df_val.reset_index(drop=True), \n    embeddings_val, \n    molecular_features_val\n], axis=1)\ny_val = df_ttl_val['TmS']\nX_val = df_ttl_val.drop(df_val.columns, axis=1)\n#X_val = df_ttl_val.drop(['id', 'Tm', 'TmS', 'SMILES'], axis=1)\nX_val.columns = [str(colname) for colname in X_val.columns]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T14:33:39.715331Z","iopub.execute_input":"2025-10-23T14:33:39.715624Z","iopub.status.idle":"2025-10-23T14:33:39.752884Z","shell.execute_reply.started":"2025-10-23T14:33:39.715596Z","shell.execute_reply":"2025-10-23T14:33:39.752236Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Extreme Gradient Boosting","metadata":{}},{"cell_type":"code","source":"# Define hyperparameter grid for optimization  \nparam_grid = {\n    'n_estimators' : [200, 500, 700, 1000],\n    'max_depth': [3, 7],\n    'eta': [0.05, 0.1, 0.15],\n    'subsample': [0.7],\n    'colsample_bytree': [0.8]\n}\nmae_scores = []\n\nfor n_estimators in param_grid['n_estimators']:\n    for max_depth in param_grid['max_depth']:\n        for eta in param_grid['eta']:\n            for subsample in param_grid['subsample']:\n                for colsample_bytree in param_grid['colsample_bytree']:\n                    model_xgb = xgb.XGBRegressor(n_estimators=n_estimators, max_depth=max_depth, eta=eta, subsample=subsample, colsample_bytree=colsample_bytree, random_state=seed)\n                    model_xgb.fit(X_train, y_train)\n                    y_pred = model_xgb.predict(X_val)\n                    mae_score = mean_absolute_error(y_val, y_pred) * std\n                    mae_scores.append([n_estimators, max_depth, eta, subsample, colsample_bytree, mae_score])\n                    \ndf_downstream = pd.DataFrame(mae_scores, columns=['n_estimators', 'max_depth', 'eta', 'subsample', 'colsample_bytree', 'mae_score']) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T14:33:39.753693Z","iopub.execute_input":"2025-10-23T14:33:39.753946Z","iopub.status.idle":"2025-10-23T14:42:52.462738Z","shell.execute_reply.started":"2025-10-23T14:33:39.753928Z","shell.execute_reply":"2025-10-23T14:42:52.461926Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_downstream.sort_values(by='mae_score', inplace=True)\ndf_downstream.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T14:42:52.463544Z","iopub.execute_input":"2025-10-23T14:42:52.463823Z","iopub.status.idle":"2025-10-23T14:42:52.474511Z","shell.execute_reply.started":"2025-10-23T14:42:52.463799Z","shell.execute_reply":"2025-10-23T14:42:52.473754Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Retrieve the best model parameters\nbest_model_config = df_downstream.nsmallest(1, 'mae_score').\\\n    drop('mae_score', axis=1).\\\n    iloc[0, :].to_dict()\n\n# Type correction\nbest_model_config['max_depth'] = int(best_model_config['max_depth'])\nbest_model_config['n_estimators'] = int(best_model_config['n_estimators'])\n\n# Train the downstream model with the best parameters\nbest_model = xgb.XGBRegressor(random_state=seed, **best_model_config)\nbest_model.fit(X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T14:42:52.475457Z","iopub.execute_input":"2025-10-23T14:42:52.476153Z","iopub.status.idle":"2025-10-23T14:44:04.997685Z","shell.execute_reply.started":"2025-10-23T14:42:52.476127Z","shell.execute_reply":"2025-10-23T14:44:04.996817Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"# Generate features for the test set\nembeddings_test = extract_chembert_embeddings(df_test['SMILES'], chemberta, df_test.shape[0])\nmolecular_features_test = extract_all_descriptors(df_test['SMILES'])\n\n# the best downstream regression model\n# model = ElasticNet(random_state=seed, alpha=0.03, l1_ratio=0.1)\n# model.fit(X_train, y_train)\n\n# Prepare dataset for downstream regression tasks\ndf_ttl_test = pd.concat([\n    df_test.reset_index(drop=True), \n    embeddings_test, \n    molecular_features_test\n], axis=1)\n\n\nX_test = df_ttl_test.drop(df_test.columns, axis=1)\nX_test.columns = [str(colname) for colname in X_test.columns]\n\ny_pred = scaler.inverse_transform( # back transform to raw target\n    np.expand_dims(best_model.predict(X_test), axis=1) # convert to 2D array by adding an extra dimension\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T14:45:43.853942Z","iopub.execute_input":"2025-10-23T14:45:43.854697Z","iopub.status.idle":"2025-10-23T14:45:51.511966Z","shell.execute_reply.started":"2025-10-23T14:45:43.854674Z","shell.execute_reply":"2025-10-23T14:45:51.510972Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_out = pd.DataFrame({'id': df_ttl_test['id'],'Tm': y_pred.flatten()})\ndf_out.to_csv('./submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T14:45:58.036219Z","iopub.execute_input":"2025-10-23T14:45:58.036537Z","iopub.status.idle":"2025-10-23T14:45:58.049903Z","shell.execute_reply.started":"2025-10-23T14:45:58.036517Z","shell.execute_reply":"2025-10-23T14:45:58.049179Z"}},"outputs":[],"execution_count":null}]}