{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":113155,"databundleVersionId":13473948,"sourceType":"competition"},{"sourceId":442871,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":359690,"modelId":380893}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Packages","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\n \nfrom sklearn.linear_model import ElasticNet \nfrom sklearn.model_selection import KFold, GridSearchCV  \nfrom torch.optim import AdamW\nfrom torch.utils.data import Dataset, DataLoader   \nfrom transformers import AutoModelForMaskedLM, AutoTokenizer, AutoModel, AutoModelForSequenceClassification","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T15:37:37.023567Z","iopub.execute_input":"2025-10-16T15:37:37.023745Z","iopub.status.idle":"2025-10-16T15:37:50.645645Z","shell.execute_reply.started":"2025-10-16T15:37:37.023730Z","shell.execute_reply":"2025-10-16T15:37:50.644795Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Dataset and Pretrained Model","metadata":{}},{"cell_type":"markdown","source":"* The training dataset `train.csv` is loaded to `df_train`.\n* The transformer model `ChemBerta` is available in Kaggle (https://www.kaggle.com/code/michaelrowen/opp2025-chemberta-pre-trained-base).","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/melting-point/train.csv')\nchemberta_model = '/kaggle/input/c/transformers/default/1/ChemBERTa-77M-MLM'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T15:37:50.647349Z","iopub.execute_input":"2025-10-16T15:37:50.648052Z","iopub.status.idle":"2025-10-16T15:37:50.774432Z","shell.execute_reply.started":"2025-10-16T15:37:50.648033Z","shell.execute_reply":"2025-10-16T15:37:50.773805Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nTm_Scaler = StandardScaler()\nTm_Scaler.fit(df_train[['Tm']])\ndf_train['TmS'] = Tm_Scaler.transform(df_train[['Tm']]).flatten()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T17:02:41.175019Z","iopub.execute_input":"2025-10-16T17:02:41.175774Z","iopub.status.idle":"2025-10-16T17:02:41.183963Z","shell.execute_reply.started":"2025-10-16T17:02:41.175748Z","shell.execute_reply":"2025-10-16T17:02:41.183228Z"}},"outputs":[],"execution_count":130},{"cell_type":"markdown","source":"Define the data handler for fine-tuning ChemBerta with pytorch.","metadata":{}},{"cell_type":"code","source":"class ChemDataset(Dataset):  \n    def __init__(self, df, tokenizer, max_length=128):  \n        self.smiles = df['SMILES'].tolist()  \n        self.labels = df['TmS'].tolist()  \n        self.tokenizer = tokenizer  \n        self.max_length = max_length  \n  \n    def __len__(self):  \n        return len(self.labels)  \n  \n    def __getitem__(self, idx):  \n        encoding = self.tokenizer(  \n            self.smiles[idx],  \n            truncation=True,  \n            padding='max_length',  \n            max_length=self.max_length,  \n            return_tensors='pt'  \n        )  \n        item = {key: val.squeeze(0) for key, val in encoding.items()}  \n        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)  \n        return item  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T17:06:06.611373Z","iopub.execute_input":"2025-10-16T17:06:06.612019Z","iopub.status.idle":"2025-10-16T17:06:06.618800Z","shell.execute_reply.started":"2025-10-16T17:06:06.611988Z","shell.execute_reply":"2025-10-16T17:06:06.618062Z"}},"outputs":[],"execution_count":140},{"cell_type":"markdown","source":"Define the transformer model ChemBerta.","metadata":{}},{"cell_type":"code","source":"# Model retrieved from https://www.kaggle.com/code/michaelrowen/opp2025-chemberta-pre-trained-base\nclass BERTEmbedder:\n    def __init__(self, model_name):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForSequenceClassification.from_pretrained(  \n            model_name,\n            num_labels = 1,  # Regression task  \n            problem_type = 'regression'\n        )  \n        self.model.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T17:06:08.650609Z","iopub.execute_input":"2025-10-16T17:06:08.650872Z","iopub.status.idle":"2025-10-16T17:06:08.655417Z","shell.execute_reply.started":"2025-10-16T17:06:08.650851Z","shell.execute_reply":"2025-10-16T17:06:08.654513Z"}},"outputs":[],"execution_count":141},{"cell_type":"markdown","source":"Load the transfomer model.","metadata":{}},{"cell_type":"code","source":"chemberta = BERTEmbedder(model_name=chemberta_model)\noptimizer = AdamW(chemberta.model.parameters(), lr=5e-4)\nloss_fn = nn.L1Loss()\n\ndataset = ChemDataset(df_train, chemberta.tokenizer)\ndataloader = DataLoader(dataset, batch_size=16, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T17:06:09.798235Z","iopub.execute_input":"2025-10-16T17:06:09.798525Z","iopub.status.idle":"2025-10-16T17:06:09.909784Z","shell.execute_reply.started":"2025-10-16T17:06:09.798504Z","shell.execute_reply":"2025-10-16T17:06:09.908989Z"}},"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/c/transformers/default/1/ChemBERTa-77M-MLM and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":142},{"cell_type":"markdown","source":"The unit test for creating and loading `ChemDataset` in development. ","metadata":{}},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'  \nchemberta.model.to(device)\n\nn_epochs = 50\nfor epoch in range(n_epochs):\n    chemberta.model.train()\n    epoch_loss = 0\n    epoch_size = 0\n    for batch in dataloader:\n        inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}  \n        labels = batch['labels'].to(device).unsqueeze(1)  # shape [B,1]  \n        outputs = chemberta.model(**inputs).logits  # shape [B,1] \n        loss = loss_fn(outputs, labels)  \n        optimizer.zero_grad()  \n        loss.backward()  \n        optimizer.step()  \n        epoch_loss += loss.item() * dataloader.batch_size * (Tm_Scaler.var_[0]**0.5)\n        epoch_size += dataloader.batch_size\n    \n    print(f\"Epoch {epoch + 1} done, last batch loss: {epoch_loss/epoch_size:.4f}\")  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T17:06:13.222411Z","iopub.execute_input":"2025-10-16T17:06:13.222977Z","iopub.status.idle":"2025-10-16T17:09:02.048949Z","shell.execute_reply.started":"2025-10-16T17:06:13.222951Z","shell.execute_reply":"2025-10-16T17:09:02.048298Z"}},"outputs":[{"name":"stdout","text":"Epoch 1 done, last batch loss: 45.9048\nEpoch 2 done, last batch loss: 39.5324\nEpoch 3 done, last batch loss: 36.4212\nEpoch 4 done, last batch loss: 34.5219\nEpoch 5 done, last batch loss: 33.0328\nEpoch 6 done, last batch loss: 32.2097\nEpoch 7 done, last batch loss: 30.9195\nEpoch 8 done, last batch loss: 30.4775\nEpoch 9 done, last batch loss: 29.8145\nEpoch 10 done, last batch loss: 29.5090\nEpoch 11 done, last batch loss: 28.8498\nEpoch 12 done, last batch loss: 28.0860\nEpoch 13 done, last batch loss: 27.5519\nEpoch 14 done, last batch loss: 26.8073\nEpoch 15 done, last batch loss: 26.5618\nEpoch 16 done, last batch loss: 26.5693\nEpoch 17 done, last batch loss: 26.5914\nEpoch 18 done, last batch loss: 26.1552\nEpoch 19 done, last batch loss: 25.4796\nEpoch 20 done, last batch loss: 25.3421\nEpoch 21 done, last batch loss: 24.9859\nEpoch 22 done, last batch loss: 24.2280\nEpoch 23 done, last batch loss: 23.7546\nEpoch 24 done, last batch loss: 23.7413\nEpoch 25 done, last batch loss: 25.6481\nEpoch 26 done, last batch loss: 23.5511\nEpoch 27 done, last batch loss: 23.5849\nEpoch 28 done, last batch loss: 22.5992\nEpoch 29 done, last batch loss: 22.6952\nEpoch 30 done, last batch loss: 24.2913\nEpoch 31 done, last batch loss: 22.8756\nEpoch 32 done, last batch loss: 22.6625\nEpoch 33 done, last batch loss: 21.1426\nEpoch 34 done, last batch loss: 22.6399\nEpoch 35 done, last batch loss: 21.8676\nEpoch 36 done, last batch loss: 21.3029\nEpoch 37 done, last batch loss: 20.8982\nEpoch 38 done, last batch loss: 21.6335\nEpoch 39 done, last batch loss: 21.3966\nEpoch 40 done, last batch loss: 21.2533\nEpoch 41 done, last batch loss: 20.3636\nEpoch 42 done, last batch loss: 20.3910\nEpoch 43 done, last batch loss: 21.1844\nEpoch 44 done, last batch loss: 20.4258\nEpoch 45 done, last batch loss: 20.6205\nEpoch 46 done, last batch loss: 20.4116\nEpoch 47 done, last batch loss: 19.7872\nEpoch 48 done, last batch loss: 19.7492\nEpoch 49 done, last batch loss: 19.4606\nEpoch 50 done, last batch loss: 18.7930\n","output_type":"stream"}],"execution_count":143},{"cell_type":"code","source":"chemberta.model.to('cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T17:09:05.901640Z","iopub.execute_input":"2025-10-16T17:09:05.902193Z","iopub.status.idle":"2025-10-16T17:09:05.928785Z","shell.execute_reply.started":"2025-10-16T17:09:05.902165Z","shell.execute_reply":"2025-10-16T17:09:05.927945Z"}},"outputs":[{"execution_count":144,"output_type":"execute_result","data":{"text/plain":"RobertaForSequenceClassification(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(600, 384, padding_idx=1)\n      (position_embeddings): Embedding(515, 384, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 384)\n      (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.144, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-2): 3 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSdpaSelfAttention(\n              (query): Linear(in_features=384, out_features=384, bias=True)\n              (key): Linear(in_features=384, out_features=384, bias=True)\n              (value): Linear(in_features=384, out_features=384, bias=True)\n              (dropout): Dropout(p=0.109, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=384, out_features=384, bias=True)\n              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.144, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=384, out_features=464, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=464, out_features=384, bias=True)\n            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.144, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): RobertaClassificationHead(\n    (dense): Linear(in_features=384, out_features=384, bias=True)\n    (dropout): Dropout(p=0.144, inplace=False)\n    (out_proj): Linear(in_features=384, out_features=1, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":144},{"cell_type":"code","source":"def extract_chembert_embeddings(smiles_list, embedder, n_data):\n    n_latent = 384\n    embeddings = np.zeros((n_data, n_latent))\n    \n    for i, smiles in enumerate(smiles_list):\n        with torch.no_grad():\n            # Getting the model output\n            encoded_input = embedder.tokenizer(smiles, return_tensors='pt', padding=True, truncation=True)\n            model_output = embedder.model(**encoded_input, output_hidden_states=True)\n            \n            # Getting the CLS token from model output\n            embedding = model_output.hidden_states[3][:, 0, :]\n            embeddings[i, :] = embedding.numpy()\n    \n    return pd.DataFrame(embeddings)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T17:09:10.194731Z","iopub.execute_input":"2025-10-16T17:09:10.195428Z","iopub.status.idle":"2025-10-16T17:09:10.200292Z","shell.execute_reply.started":"2025-10-16T17:09:10.195404Z","shell.execute_reply":"2025-10-16T17:09:10.199606Z"}},"outputs":[],"execution_count":145},{"cell_type":"code","source":"# Code retrieved from https://www.kaggle.com/code/michaelrowen/opp2025-chemberta-pre-trained-base\ndef extract_simple_molecular_features(smiles_list):\n    features = []\n    for smiles in smiles_list:\n        feature_vector = [\n            len(smiles),  # SMILES length\n            smiles.count('C'),  # Carbon count\n            smiles.count('N'),  # Nitrogen count\n            smiles.count('O'),  # Oxygen count\n            smiles.count('S'),  # Sulfur count\n            smiles.count('P'),  # Phosphorus count\n            smiles.count('F'),  # Fluorine count\n            smiles.count('Cl'),  # Chlorine count\n            smiles.count('Br'),  # Bromine count\n            smiles.count('I'),  # Iodine count\n            smiles.count('='),  # Double bonds\n            smiles.count('#'),  # Triple bonds\n            smiles.count('-'),  # Single bonds\n            smiles.count('(') + smiles.count(')'),  # Branching\n            smiles.count('[') + smiles.count(']'),  # Bracket atoms\n            smiles.count('@'),  # Chirality centers\n            smiles.count('c'),  # Aromatic carbon\n            smiles.count('n'),  # Aromatic nitrogen\n            smiles.count('o'),  # Aromatic oxygen\n            smiles.count('s'),  # Aromatic sulfur\n        ]\n        features.append(feature_vector)\n    \n    return pd.DataFrame(features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T17:09:11.791046Z","iopub.execute_input":"2025-10-16T17:09:11.791764Z","iopub.status.idle":"2025-10-16T17:09:11.797088Z","shell.execute_reply.started":"2025-10-16T17:09:11.791736Z","shell.execute_reply":"2025-10-16T17:09:11.796415Z"}},"outputs":[],"execution_count":146},{"cell_type":"code","source":"embeddings_train = extract_chembert_embeddings(df_train['SMILES'], chemberta, df_train.shape[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T17:09:32.728330Z","iopub.execute_input":"2025-10-16T17:09:32.729148Z","iopub.status.idle":"2025-10-16T17:09:49.899666Z","shell.execute_reply.started":"2025-10-16T17:09:32.729117Z","shell.execute_reply":"2025-10-16T17:09:49.898779Z"}},"outputs":[],"execution_count":147},{"cell_type":"code","source":"#embeddings_train = extract_chembert_embeddings(df_train['SMILES'], chemberta, df_train.shape[0])\nmolecular_features_train = extract_simple_molecular_features(df_train['SMILES'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T17:09:49.900815Z","iopub.execute_input":"2025-10-16T17:09:49.901042Z","iopub.status.idle":"2025-10-16T17:09:49.922443Z","shell.execute_reply.started":"2025-10-16T17:09:49.901026Z","shell.execute_reply":"2025-10-16T17:09:49.921896Z"}},"outputs":[],"execution_count":148},{"cell_type":"code","source":"print(embeddings_train.shape, type(embeddings_train))\nprint(molecular_features_train.shape, type(molecular_features_train))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T17:09:49.923028Z","iopub.execute_input":"2025-10-16T17:09:49.923233Z","iopub.status.idle":"2025-10-16T17:09:49.938066Z","shell.execute_reply.started":"2025-10-16T17:09:49.923218Z","shell.execute_reply":"2025-10-16T17:09:49.937532Z"}},"outputs":[{"name":"stdout","text":"(2662, 384) <class 'pandas.core.frame.DataFrame'>\n(2662, 20) <class 'pandas.core.frame.DataFrame'>\n","output_type":"stream"}],"execution_count":149},{"cell_type":"code","source":"df_ttl = pd.concat([df_train, embeddings_train, molecular_features_train], axis=1)\ny_train = df_ttl['TmS']\n# X_train = df_ttl.drop(df_train.columns, axis=1)\nX_train = df_ttl.drop(['id', 'Tm', 'TmS', 'SMILES'], axis=1)\nX_train.columns = [str(colname) for colname in X_train.columns]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T17:10:20.767269Z","iopub.execute_input":"2025-10-16T17:10:20.767528Z","iopub.status.idle":"2025-10-16T17:10:20.787918Z","shell.execute_reply.started":"2025-10-16T17:10:20.767510Z","shell.execute_reply":"2025-10-16T17:10:20.787202Z"}},"outputs":[],"execution_count":150},{"cell_type":"code","source":"# from sklearn.ensemble import GradientBoostingRegressor\nseed = 20251017\n\n# Initialize the classifier  \n#model = RandomForestRegressor(n_jobs=-1)\nmodel = ElasticNet()\n\n# Define hyperparameter grid for optimization  \nparam_grid = {\n    'alpha' : [0.1, 0.5, 1, 5, 50, 100, 200],\n    'l1_ratio': [0.25, 0.5, 0.75]\n}  \n\n# Set up cross-validation  \ncv = KFold(n_splits=10, shuffle=True, random_state=seed)  \n  \n# Set up GridSearchCV  \ngs = GridSearchCV(  \n    estimator=model,  \n    param_grid=param_grid,  \n    cv=cv,  \n    scoring='neg_mean_absolute_error',  \n    verbose=1,  \n    n_jobs=-1  \n)  \n  \n# Fit the model with categorical feature information  \ngs.fit(X_train, y_train)  \n  \n# Print the best hyperparameters and score  \nprint(\"Best hyperparameters:\", gs.best_params_)  \nprint(\"Best CV accuracy:\", gs.best_score_ * (Tm_Scaler.var_[0]**0.5))  \n  \n# Get the best model  \nbest_model = gs.best_estimator_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T17:12:35.235828Z","iopub.execute_input":"2025-10-16T17:12:35.236377Z","iopub.status.idle":"2025-10-16T17:12:42.198580Z","shell.execute_reply.started":"2025-10-16T17:12:35.236355Z","shell.execute_reply":"2025-10-16T17:12:42.197706Z"}},"outputs":[{"name":"stdout","text":"Fitting 10 folds for each of 21 candidates, totalling 210 fits\nBest hyperparameters: {'alpha': 0.1, 'l1_ratio': 0.25}\nBest CV accuracy: -26.33931911040452\n","output_type":"stream"}],"execution_count":152},{"cell_type":"code","source":"best_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T17:12:46.138946Z","iopub.execute_input":"2025-10-16T17:12:46.139436Z","iopub.status.idle":"2025-10-16T17:12:46.144593Z","shell.execute_reply.started":"2025-10-16T17:12:46.139416Z","shell.execute_reply":"2025-10-16T17:12:46.143909Z"}},"outputs":[{"execution_count":154,"output_type":"execute_result","data":{"text/plain":"ElasticNet(alpha=0.1, l1_ratio=0.25)","text/html":"<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ElasticNet(alpha=0.1, l1_ratio=0.25)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ElasticNet</label><div class=\"sk-toggleable__content\"><pre>ElasticNet(alpha=0.1, l1_ratio=0.25)</pre></div></div></div></div></div>"},"metadata":{}}],"execution_count":154},{"cell_type":"code","source":"df_test = pd.read_csv('/kaggle/input/melting-point/test.csv')\nembeddings_test = extract_chembert_embeddings(df_test['SMILES'], chemberta, df_test.shape[0])\nmolecular_features_test = extract_simple_molecular_features(df_test['SMILES'])\n\ndf_ttl = pd.concat([df_test, embeddings_test, molecular_features_test], axis=1)\n# X_test = df_ttl.drop(df_test.columns, axis=1)\nX_test = df_ttl.drop(['id', 'SMILES'], axis=1)\nX_test.columns = [str(colname) for colname in X_test.columns]\n\ny_pred = Tm_Scaler.inverse_transform(np.expand_dims(best_model.predict(X_test), axis=1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T17:16:38.845344Z","iopub.execute_input":"2025-10-16T17:16:38.845618Z","iopub.status.idle":"2025-10-16T17:16:43.187667Z","shell.execute_reply.started":"2025-10-16T17:16:38.845595Z","shell.execute_reply":"2025-10-16T17:16:43.186877Z"}},"outputs":[],"execution_count":161},{"cell_type":"code","source":"df_out = pd.DataFrame({'id': df_ttl['id'],'Tm': y_pred.flatten()})\ndf_out.to_csv('./submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T17:17:21.760059Z","iopub.execute_input":"2025-10-16T17:17:21.760690Z","iopub.status.idle":"2025-10-16T17:17:21.768630Z","shell.execute_reply.started":"2025-10-16T17:17:21.760666Z","shell.execute_reply":"2025-10-16T17:17:21.767890Z"}},"outputs":[],"execution_count":164}]}